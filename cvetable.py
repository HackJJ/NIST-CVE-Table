import requests
from bs4 import BeautifulSoup
import argparse
import re
from datetime import datetime, timedelta

def fetch_data(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    return soup

def is_within_months(published_date, max_months):
    current_date = datetime.now()
    date_limit = current_date - timedelta(days=max_months * 30)
    return published_date > date_limit

def parse_table(soup, max_months):
    table = soup.find('table', {'data-testid': 'vuln-results-table'})
    rows = table.find_all('tr') if table else []

    data = []
    for row in rows[1:]:
        vuln_id_cell = row.find('th')
        vuln_id = vuln_id_cell.find('a').text.strip()
        vuln_link = 'https://nvd.nist.gov' + vuln_id_cell.find('a')['href']

        date_str = row.find('span', {
            'data-testid': re.compile('vuln-published-on')
        }).text.strip()
        date_published = datetime.strptime(date_str.split(';')[0], '%B %d, %Y')

        if not is_within_months(date_published, max_months):
            return data, False

        cvss_severity = row.find('td', {'nowrap': 'nowrap'}).find('a').text.strip()

        data.append((vuln_id, vuln_link, date_str.split(';')[0], cvss_severity))

    return data, True

def generate_asciidoc_table(data):
    asciidoc = '[cols="1,1,1"]\n|===\n'
    asciidoc += '| Vuln ID | Date Published | CVSS Severity\n\n'

    for item in data:
        vuln_id, vuln_link, date_published, cvss_severity = item
        asciidoc += f'| {vuln_link}[{vuln_id}] | {date_published} | {cvss_severity}\n'

    asciidoc += '|==='
    return asciidoc

def main(url, max_months):
    base_url = url.split('?')[0]
    params = url.split('?')[1]
    data = []
    continue_loop = True

    for start_index in range(0, 1001, 20):
        if not continue_loop:
            break

        modified_url = f"{base_url}?{params}&startIndex={start_index}"
        soup = fetch_data(modified_url)
        page_data, continue_loop = parse_table(soup, max_months)
        
        data.extend(page_data)

    asciidoc_table = generate_asciidoc_table(data)
    print(asciidoc_table)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="NVD Table to AsciiDoc")
    parser.add_argument('url', type=str, help='URL of the NVD page')
    parser.add_argument('max_months', type=int, nargs='?', default=12,
                        help='Maximum age of vulnerabilities in months (default: 12)')
    args = parser.parse_args()
    main(args.url, args.max_months)
